{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ucl-casa-ce/casa0018/blob/main/Week6/CASA0018_6_ObjectDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WoIdEeJJuol"
      },
      "source": [
        "# Object Detection example\n",
        "\n",
        "This Colab runs through a basic example of creating an object detector that will load an image and highlight known objects. It uses the COCO dataset and a number of pretrained models from the TF2 [Object Detection Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md).\n",
        "\n",
        "Example based on the [TensorFlow tutorial](https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/tf2_object_detection.ipynb#scrollTo=-y9R0Xllefec) and the example [here](https://www.analyticsvidhya.com/blog/2020/04/build-your-own-object-detection-model-using-tensorflow-api/).  \n",
        "\n",
        "To start we import our dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXge-anmJqzK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pathlib\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import io\n",
        "import scipy.misc\n",
        "import numpy as np\n",
        "from six import BytesIO\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from six.moves.urllib.request import urlopen\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwClYgK9-Yc8"
      },
      "source": [
        "Clone the model repo into our contents folder over on the left."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvoRn36UViCw"
      },
      "outputs": [],
      "source": [
        "# Clone the tensorflow models repository\n",
        "!git clone --depth 1 https://github.com/tensorflow/models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5mbrlUZMM87"
      },
      "source": [
        "We need to setup the object detection packages ready for use - to do this we compile the protobufs - (for those interested more info is available here:  https://developers.google.com/protocol-buffers/docs/pythontutorial) and install the object_detection package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RiLOMxQjKfqi"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "sudo apt install -y protobuf-compiler\n",
        "cd models/research/\n",
        "protoc object_detection/protos/*.proto --python_out=.\n",
        "cp object_detection/packages/tf2/setup.py .\n",
        "python -m pip install ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYswVKXx_RUY"
      },
      "source": [
        "And then load in some dependencies from the object_detection packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5S1Gao5RbRw"
      },
      "outputs": [],
      "source": [
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "from object_detection.utils import ops as utils_ops\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3N4ZQ_RZ9ir6"
      },
      "source": [
        "Next up define a helper function to simplify loading in image files and define the list of models we want to expose from the model zoo (note this is a subset of those available, add more here if you want them to be added to the drop down menu further down the Colab). List of tuples with Human Keypoints for the COCO 2017 dataset. This is needed for models with keypoints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NT_yAACYUix5"
      },
      "outputs": [],
      "source": [
        "def load_image_into_numpy_array(path):\n",
        "  \"\"\"Load an image from file into a numpy array.\n",
        "\n",
        "  Puts image into numpy array to feed into tensorflow graph.\n",
        "  Note that by convention we put it into a numpy array with shape\n",
        "  (height, width, channels), where channels=3 for RGB.\n",
        "\n",
        "  Args:\n",
        "    path: the file path to the image\n",
        "\n",
        "  Returns:\n",
        "    uint8 numpy array with shape (img_height, img_width, 3)\n",
        "  \"\"\"\n",
        "  image = None\n",
        "  if(path.startswith('http')):\n",
        "    response = urlopen(path)\n",
        "    image_data = response.read()\n",
        "    image_data = BytesIO(image_data)\n",
        "    image = Image.open(image_data)\n",
        "  else:\n",
        "    image_data = tf.io.gfile.GFile(path, 'rb').read()\n",
        "    image = Image.open(BytesIO(image_data))\n",
        "\n",
        "  (im_width, im_height) = image.size\n",
        "  return np.array(image.getdata()).reshape(\n",
        "      (1, im_height, im_width, 3)).astype(np.uint8)\n",
        "\n",
        "ALL_MODELS = {\n",
        "'CenterNet HourGlass104 512x512' : 'https://tfhub.dev/tensorflow/centernet/hourglass_512x512/1',\n",
        "'CenterNet HourGlass104 Keypoints 512x512' : 'https://tfhub.dev/tensorflow/centernet/hourglass_512x512_kpts/1',\n",
        "'CenterNet Resnet50 V1 FPN 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v1_fpn_512x512/1',\n",
        "'CenterNet Resnet50 V2 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v2_512x512/1',\n",
        "'EfficientDet D0 512x512' : 'https://tfhub.dev/tensorflow/efficientdet/d0/1',\n",
        "'SSD MobileNet v2 320x320' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v2/2',\n",
        "'SSD MobileNet V2 FPNLite 320x320' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v2/fpnlite_320x320/1',\n",
        "'SSD ResNet50 V1 FPN 640x640 (RetinaNet50)' : 'https://tfhub.dev/tensorflow/retinanet/resnet50_v1_fpn_640x640/1',\n",
        "'SSD ResNet101 V1 FPN 640x640 (RetinaNet101)' : 'https://tfhub.dev/tensorflow/retinanet/resnet101_v1_fpn_640x640/1',\n",
        "'SSD ResNet152 V1 FPN 640x640 (RetinaNet152)' : 'https://tfhub.dev/tensorflow/retinanet/resnet152_v1_fpn_640x640/1',\n",
        "'Faster R-CNN ResNet50 V1 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_640x640/1',\n",
        "'Faster R-CNN ResNet101 V1 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet101_v1_640x640/1',\n",
        "'Faster R-CNN ResNet152 V1 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet152_v1_640x640/1',\n",
        "'Faster R-CNN Inception ResNet V2 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/inception_resnet_v2_640x640/1',\n",
        "'Mask R-CNN Inception ResNet V2 1024x1024' : 'https://tfhub.dev/tensorflow/mask_rcnn/inception_resnet_v2_1024x1024/1'\n",
        "\n",
        "}\n",
        "\n",
        "COCO17_HUMAN_POSE_KEYPOINTS = [(0, 1),\n",
        " (0, 2),\n",
        " (1, 3),\n",
        " (2, 4),\n",
        " (0, 5),\n",
        " (0, 6),\n",
        " (5, 7),\n",
        " (7, 9),\n",
        " (6, 8),\n",
        " (8, 10),\n",
        " (5, 6),\n",
        " (5, 11),\n",
        " (6, 12),\n",
        " (11, 12),\n",
        " (11, 13),\n",
        " (13, 15),\n",
        " (12, 14),\n",
        " (14, 16)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "co96cj5W_luo"
      },
      "source": [
        "The code below creates an array of the labels used in the models so that we can associate a class number to a human readable label - e.g. \"person\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khen2uKsWKoU"
      },
      "outputs": [],
      "source": [
        "PATH_TO_LABELS = './models/research/object_detection/data/mscoco_label_map.pbtxt'\n",
        "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rmq8AlJAOZp"
      },
      "source": [
        "Select the model type to be used.\n",
        "\n",
        "Tip: if you want to read more details about the selected model, you can follow the link (model handle) and read aditional documentation on TF Hub. After you select a model, we will print the handle to make it easier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2NJUVGjWShg",
        "outputId": "d05cf60d-88b7-4ab6-ab6a-266f6be7c2fe"
      },
      "outputs": [],
      "source": [
        "model_display_name = \"Faster R-CNN ResNet50 V1 640x640\" #@param [\"CenterNet HourGlass104 512x512\", \"CenterNet HourGlass104 Keypoints 512x512\", \"CenterNet Resnet50 V1 FPN 512x512\", \"CenterNet Resnet50 V2 512x512\", \"EfficientDet D0 512x512\", \"SSD MobileNet v2 320x320\", \"SSD MobileNet V2 FPNLite 320x320\", \"SSD ResNet50 V1 FPN 640x640 (RetinaNet50)\", \"SSD ResNet101 V1 FPN 640x640 (RetinaNet101)\", \"SSD ResNet152 V1 FPN 640x640 (RetinaNet152)\", \"Faster R-CNN ResNet50 V1 640x640\", \"Faster R-CNN ResNet101 V1 640x640\", \"Faster R-CNN ResNet152 V1 640x640\", \"Faster R-CNN Inception ResNet V2 640x640\", \"Mask R-CNN Inception ResNet V2 1024x1024\"]\n",
        "model_handle = ALL_MODELS[model_display_name]\n",
        "\n",
        "print('Selected model:'+ model_display_name)\n",
        "print('Model Handle at TensorFlow Hub: {}'.format(model_handle))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gtveyt6xB6HM"
      },
      "source": [
        "Next up we load pre-trained model weights from the TensorFlow Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNMM1NNYh3hu",
        "outputId": "d878899b-4c61-4460-9714-b7502bfd8cc6"
      },
      "outputs": [],
      "source": [
        "print('loading model...')\n",
        "hub_model = hub.load(model_handle)\n",
        "print('model loaded!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jc7E9qwiBIHN"
      },
      "source": [
        "The code below runs for each image inference. It takes an image, converts it into an input tensor for the model and outputs results based on the type of model being used. To inspect what keys are available for the model used print out `output_dict.keys()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhKf3z05b91K"
      },
      "outputs": [],
      "source": [
        "def run_inference_for_single_image(model, image):\n",
        "  image = np.asarray(image)\n",
        "  # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
        "  input_tensor = tf.convert_to_tensor(image)\n",
        "  # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
        "  input_tensor = input_tensor[tf.newaxis,...]\n",
        "\n",
        "  # Run inference\n",
        "  output_dict = model(input_tensor)\n",
        "\n",
        "  # All outputs are batches tensors.\n",
        "  # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
        "  # We're only interested in the first num_detections.\n",
        "  num_detections = int(output_dict.pop('num_detections'))\n",
        "  output_dict = {key:value[0, :num_detections].numpy() \n",
        "                 for key,value in output_dict.items()}\n",
        "  output_dict['num_detections'] = num_detections\n",
        "\n",
        "  #print(output_dict.keys())\n",
        "  #print(output_dict['num_detections'])\n",
        "\n",
        "  # detection_classes should be ints.\n",
        "  output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
        "   \n",
        "  # Handle models with masks:\n",
        "  if 'detection_masks' in output_dict:\n",
        "    # Reframe the the bbox mask to the image size.\n",
        "    detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
        "              output_dict['detection_masks'], output_dict['detection_boxes'],\n",
        "               image.shape[0], image.shape[1])      \n",
        "    detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5,\n",
        "                                       tf.uint8)\n",
        "    output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
        "    \n",
        "  return output_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ER0JcJb9JmkA"
      },
      "source": [
        "Show_inference calls the inference method above and visualises the results. It uses visualisation_utils functions which can be found at https://github.com/tensorflow/models/blob/master/research/object_detection/utils/visualization_utils.py  \n",
        "\n",
        "Experiment with changing the max_boxes_to_draw and min_score_thresh to see how results are displayed. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYQbMIEWcD4m"
      },
      "outputs": [],
      "source": [
        "def show_inference(model, image_path):\n",
        "  # the array based representation of the image will be used later in order to prepare the\n",
        "  # result image with boxes and labels on it.\n",
        "  image_np = np.array(Image.open(image_path))\n",
        "  # Actual detection.\n",
        "  output_dict = run_inference_for_single_image(model, image_np)\n",
        "\n",
        "  # Visualization of the results of a detection.\n",
        "  viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "      image_np,\n",
        "      output_dict['detection_boxes'],\n",
        "      output_dict['detection_classes'],\n",
        "      output_dict['detection_scores'],\n",
        "      category_index,\n",
        "      instance_masks=output_dict.get('detection_masks_reframed', None),\n",
        "      use_normalized_coordinates=True,\n",
        "      line_thickness=2,\n",
        "      max_boxes_to_draw=30,\n",
        "      min_score_thresh=.50)\n",
        "\n",
        "  display(Image.fromarray(image_np))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbaLO0_eKcLs"
      },
      "source": [
        "There are a couple of images built into this repo but if you want to test the code with your images, just add path to the images to the `TEST_IMAGE_PATHS`.\n",
        "\n",
        "To see what categories are in the COCO dataset to help you choose which images to use [see this repo](https://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/). \n",
        "\n",
        "\n",
        "Be careful: when using images with an alpha channel, the model expect 3 channels images and the alpha will count as a 4th."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cavBiVloe1uj",
        "outputId": "e595eed1-d890-4a3b-f70f-2c69b5c8343d"
      },
      "outputs": [],
      "source": [
        "PATH_TO_TEST_IMAGES_DIR = pathlib.Path('models/research/object_detection/test_images')\n",
        "TEST_IMAGE_PATHS = sorted(list(PATH_TO_TEST_IMAGES_DIR.glob(\"*.jpg\")))\n",
        "TEST_IMAGE_PATHS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lU7SRLR1cJ2n",
        "outputId": "3848a2ae-70ed-45b5-bc6c-a4bbce1d1462"
      },
      "outputs": [],
      "source": [
        "print('Selected model:'+ model_display_name)\n",
        "print('Model Handle at TensorFlow Hub: {}'.format(model_handle))\n",
        "\n",
        "for image_path in TEST_IMAGE_PATHS:\n",
        "  show_inference(hub_model, image_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WBAvo5nR-1m"
      },
      "source": [
        "# Whats next?\n",
        "\n",
        "Once you have a version of the object detector working start to experiment with different settings for how the data is visualised and explore how the different models create different results. Some questions you may want to ponder (and which will be useful for thinking about your final coursework) include:\n",
        "1. Which models give the best results? \n",
        "2. What does best mean? \n",
        "3. Does it change with different images? \n",
        "4. Do different models take different times to load? \n",
        "5. Why is that? \n",
        "6. Take a look at the model structures - does that explain why some take longer to load than others?"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOn+OIhXxagAQAyGwkZjany",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "CASA0018_5_ObjectDetection.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
