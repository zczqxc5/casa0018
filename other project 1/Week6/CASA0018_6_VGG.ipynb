{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/djdunc/casa0018/blob/main/Week6/CASA0018_6_VGG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntt-7kxLH4kl"
      },
      "source": [
        "# Transfer Learning \n",
        "To demonstrate the process of transfer learning we will explore an example based on VGG16. \n",
        "\n",
        "The VGG16 model was developed by the Visual Graphics Group (VGG) at Oxford and was described in the 2014 paper titled [“Very Deep Convolutional Networks for Large-Scale Image Recognition.”](https://arxiv.org/abs/1409.1556)\n",
        "\n",
        "By default, the model expects color input images to be rescaled to the size of 224×224 pixels.\n",
        "\n",
        "The pre-trained model can be loaded as follows. Running the code block will load the VGG16 model and download the model weights. Notice the last few Dense layers and the total number of trainable parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5iGSZwoHq_3"
      },
      "outputs": [],
      "source": [
        "# example of loading the vgg16 model\n",
        "from keras.applications.vgg16 import VGG16\n",
        "# load model\n",
        "model = VGG16()\n",
        "# summarize the model\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VD6SIfHkIpUl"
      },
      "source": [
        "## Exercise\n",
        "You could try the same for other models such as InceptionV3 or ResNet50. You just need to edit the lines below to explore those models. Take a look at the API documentation to see how the models are referenced - note that InceptionV3 may not be labelled quite how you first imagine and watch out for case sensitivity. \n",
        "\n",
        "https://keras.io/api/applications/resnet/\n",
        "\n",
        "https://keras.io/api/applications/inceptionv3/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oi1Onf6cKmM1"
      },
      "outputs": [],
      "source": [
        "# example of loading the resnet50 model\n",
        "from keras.applications.####ADD_IN_APPLICATION_NAME### import ResNet50\n",
        "# load model\n",
        "model = ###ADD_IN_MODEL_NAME###()\n",
        "# summarize the model\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUIT1P-gJT8-"
      },
      "outputs": [],
      "source": [
        "# example of loading the InceptionV3 model\n",
        "from keras.applications.####ADD_IN_APPLICATION_NAME### import InceptionV3\n",
        "# load model\n",
        "model = ###ADD_IN_MODEL_NAME###()\n",
        "# summarize the model\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_cYClK4DPBi"
      },
      "source": [
        "# Examples of how to use the pre-trained models "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtaRvsKeMIP-"
      },
      "source": [
        "## Simple classifer - no transfer learning.\n",
        "There is no transfer learning here but as a reminder, we can use this pre-trained model to classify an image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bN7Lv7_lM9BJ"
      },
      "outputs": [],
      "source": [
        "from keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.applications.vgg16 import decode_predictions\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "# load an image from file\n",
        "img_path = 'D_Arlo.jpg'\n",
        "img = image.load_img(img_path, target_size=(224, 224))\n",
        "# convert the image pixels to a numpy array\n",
        "i = image.img_to_array(img)\n",
        "# reshape data for the model\n",
        "i = np.expand_dims(i, axis=0)\n",
        "# prepare the image for the VGG model\n",
        "i = preprocess_input(i)\n",
        "# load the model\n",
        "model = VGG16()\n",
        "# predict the probability across all output classes\n",
        "p = model.predict(i)\n",
        "# convert the probabilities to class labels\n",
        "lst = decode_predictions(p, top=3)\n",
        "print(lst)\n",
        "\n",
        "for label in lst[0]:\n",
        "  # print(label)\n",
        "  print('I think this is a %s and I am %.2f%% confident.' % (label[1], label[2]*100))\n",
        "\n",
        "plt.imshow(img) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snDapYjOC-ZV"
      },
      "source": [
        "## Adding Transfer Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8G_saGEhDgpi"
      },
      "source": [
        "### First we will grab some data that we want to be able to classify."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdgXh24p8UH4"
      },
      "outputs": [],
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://github.com/ucl-casa-ce/casa0018/blob/main/Week6/data/validating.zip?raw=true \\\n",
        "    -O /content/validating.zip\n",
        "  \n",
        "!wget --no-check-certificate \\\n",
        "    https://github.com/ucl-casa-ce/casa0018/blob/main/Week6/data/training.zip?raw=true \\\n",
        "    -O /content/training.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "487KCrSBDz1O"
      },
      "source": [
        "Copy the data across to the content folder in the Colab Notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOdOXY9T82sz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = '/content/training.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/content/')\n",
        "zip_ref.close()\n",
        "\n",
        "local_zip = '/content/validating.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/content/')\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pOqYmCjD_7B"
      },
      "source": [
        "Show some info on the images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1FaIdtfBxhA"
      },
      "outputs": [],
      "source": [
        "mugs_dir = os.path.join('/content/training/mugs')\n",
        "plant_dir = os.path.join('/content/training/plant')\n",
        "unknown_dir = os.path.join('/content/training/unknown')\n",
        "\n",
        "print('total training mugs images:', len(os.listdir(mugs_dir)))\n",
        "print('total training plants images:', len(os.listdir(plant_dir)))\n",
        "print('total training unknown images:', len(os.listdir(unknown_dir)))\n",
        "\n",
        "mugs_files = os.listdir(mugs_dir)\n",
        "print(mugs_files[:10])\n",
        "\n",
        "plant_files = os.listdir(plant_dir)\n",
        "print(plant_files[:10])\n",
        "\n",
        "unknown_files = os.listdir(unknown_dir)\n",
        "print(unknown_files[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SJ7mJDjHLk3"
      },
      "source": [
        "Display some of the images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coprLia4Cz55"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "pic_index = 2\n",
        "\n",
        "next_mugs = [os.path.join(mugs_dir, fname) \n",
        "                for fname in mugs_files[0:pic_index]]\n",
        "next_plant = [os.path.join(plant_dir, fname) \n",
        "                for fname in plant_files[0:pic_index]]\n",
        "next_unknown = [os.path.join(unknown_dir, fname) \n",
        "                for fname in unknown_files[0:pic_index]]\n",
        "\n",
        "for i, img_path in enumerate(next_mugs+next_plant+next_unknown):\n",
        "  #print(img_path)\n",
        "  img = mpimg.imread(img_path)\n",
        "  plt.imshow(img)\n",
        "  plt.axis('Off')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBttBUrqHSE9"
      },
      "source": [
        "### Use transfer learning to create new model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYMUGX1IDeti"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras.preprocessing\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Note: ImageDataGenerator in Keras implements \"in-place\" augementation - ie it replaces\n",
        "# the batch of images past in with a transformed set. \n",
        "# This means that when the network is trained, each Epoch sees new variations of our images\n",
        "# https://www.pyimagesearch.com/2019/07/08/keras-imagedatagenerator-and-data-augmentation/ \n",
        "\n",
        "TRAINING_DIR = \"/content/training/\"\n",
        "training_datagen = ImageDataGenerator(\n",
        "      rescale = 1./255,\n",
        "\t    rotation_range=40,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        "\n",
        "VALIDATION_DIR = \"/content/validating/\"\n",
        "validation_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "print(\"Training data: \")\n",
        "train_generator = training_datagen.flow_from_directory(\n",
        "\tTRAINING_DIR,\n",
        "\ttarget_size=(150,150),\n",
        "\tclass_mode='categorical',\n",
        "  batch_size=126\n",
        ")\n",
        "\n",
        "label_map = (train_generator.class_indices)\n",
        "print(\"Classes found in data are: \", label_map)\n",
        "\n",
        "print(\"Validation data: \")\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "\tVALIDATION_DIR,\n",
        "\ttarget_size=(150,150),\n",
        "\tclass_mode='categorical',\n",
        "  batch_size=126\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5jxGFZIVapp"
      },
      "source": [
        "Next up we use a pretrained model as a starting point to build our new model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68I-TfMdloiE"
      },
      "outputs": [],
      "source": [
        "# defining a variable to hold image since since it is called in few places below\n",
        "IMAGE_SIZE = [150, 150]\n",
        "\n",
        "# load model without classifier layers\n",
        "\n",
        "pretrained_model = VGG16(weights='imagenet', \n",
        "                         include_top=False ,\n",
        "                         input_shape=[*IMAGE_SIZE, 3])\n",
        "#pretrained_model = MobileNetV2(input_shape=[*IMAGE_SIZE, 3], \n",
        "#                               include_top=False)\n",
        "#pretrained_model = ResNet50(weights='imagenet', \n",
        "#                            include_top=False, \n",
        "#                            input_shape=[*IMAGE_SIZE, 3])\n",
        "#pretrained_model = MobileNet(weights='imagenet', \n",
        "#                             include_top=False, \n",
        "#                             input_shape=[*IMAGE_SIZE, 3])\n",
        "\n",
        "pretrained_model.trainable = False\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    pretrained_model,\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss = 'categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5xSMRfMsmDP"
      },
      "source": [
        "Finaly we can run our training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENeW2ot9HUYj"
      },
      "outputs": [],
      "source": [
        "# Run training and save to history so that we can plot metrics\n",
        "history = model.fit(train_generator, \n",
        "                    epochs=20, \n",
        "                    validation_data = validation_generator, \n",
        "                    verbose = 1)\n",
        "\n",
        "# we can save the model so that we can reload at a later date\n",
        "# It appears in the content folder - download it before your session ends\n",
        "# https://www.tensorflow.org/guide/keras/save_and_serialize\n",
        "model.save(\"mugsplant.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h05-OiITri6C"
      },
      "source": [
        "Once training is complete we can plot results - this should be starting to look familiar. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_aUUG5ZJjyO"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc=0)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "\n",
        "plt.plot(epochs, loss, 'r', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
        "plt.title('Training and validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc=0)\n",
        "\n",
        "plt.tight_layout()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTvik4srQ1SS"
      },
      "source": [
        "# Testing your model\n",
        "To see how well this model has learnt you can upload an image and classify it. Some sample images have been included in the github repo in a folder called testing. You can drag and drop those into the Contents folder on left panel. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIpVfthIQxiq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from google.colab import files\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "# load an image from file\n",
        "img = image.load_img('3.jpg', target_size=(150, 150))\n",
        "\n",
        "plt.imshow(img)\n",
        "plt.show()\n",
        "\n",
        "# convert to an array that we can use as input to our model\n",
        "x = image.img_to_array(img)\n",
        "x = np.expand_dims(x, axis=0)\n",
        "images = np.vstack([x])\n",
        "\n",
        "# run prediction\n",
        "p = model.predict(images)\n",
        "  \n",
        "# create a look up of the class labels generated from the \n",
        "# \"flow_from_directory\" method used in the generator and print result\n",
        "predicted_class_indices = np.argmax(model.predict(x), axis=-1)\n",
        "labels = (train_generator.class_indices)\n",
        "labels = dict((v,k) for k,v in labels.items())\n",
        "predictions = [labels[k] for k in predicted_class_indices]\n",
        "\n",
        "print('I think the image above is \"%s\" and I am %.2f%% confident.' % (predictions[0], p[0][predicted_class_indices]*100))\n",
        "\n",
        "print(p[0])\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "8G_saGEhDgpi"
      ],
      "name": "CASA0018_5_VGG.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}